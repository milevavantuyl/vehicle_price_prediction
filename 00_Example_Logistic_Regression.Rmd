---
title: "Logistic_Regression"
author: "Marina Sanchez"
date: "5/13/2022"
output: html_document
---

```{r}
cars <- read_csv("data/cars_preprocessed.csv")

head(cars)

cars$Make <- as.factor(cars$Make)
cars$fuel <- as.factor(cars$fuel)
cars$seller_type <- as.factor(cars$seller_type)
cars$transmission <- as.factor(cars$transmission)
cars$owner <- as.factor(cars$owner)
```
#Transform response selling_price into dollars
```{r}
# 1rupee = 0,013$
cars$selling_price = 0.013*cars$selling_price
head(cars)
```

```{r}
summary(cars)
```

#Transform response selling_price into binary
```{r}
#Transformation:
#values over $10000: 0 (No)
#values below $10000: 1 (Yes)

THRESHOLD = 10000

cars["selling_price"][cars["selling_price"] <= THRESHOLD]  <- 1
cars["selling_price"][cars["selling_price"] > THRESHOLD]  <- 0

cars$selling_price = as.character(cars$selling_price)

cars["selling_price"][cars["selling_price"] == "1"]  <- "Yes"
cars["selling_price"][cars["selling_price"] == "0"]  <- "No"

cars$selling_price <- as.factor(cars$selling_price)

summary(cars)
```

# Fit an initial multiple logistic regression with all of the predictors 
```{r}
glm.multiple <- glm(selling_price ~ .,
                    data=cars,
                    family="binomial")

summary(glm.multiple)
```
We are gettin the Warning "glm.fit: fitted probabilities numerically 0 or 1 occurred". This might be happening due to extreme outliers or influential observations.

# Deal with outliers
```{r}
## Influence via Cook's distance:
cooks.distance(glm.multiple)
plot(glm.multiple, which=4)
```
```{r}
#Removing observations 142, 2625 and 6896
cars <- cars[-c(142, 2625, 6896), ]

#Fit Logistic regression again
glm.multiple <- glm(selling_price ~ .,
                    data=cars,
                    family="binomial")
glm.multiple

summary(glm.multiple)
```



There are some predictors that look to be significant: MakeTata, km_driven, seller_typeTrustmark Dealer, transmissionManual, ownerSecond Owner, mileage, max_power, seats and age   

# Using VIF to remove colinearity
```{r}
library(car)

vif(glm.multiple)
```
# We are removing Make since it has the highest VIF
```{r}
cars <- cars %>% select(-Make)

glm.multiple_red <- glm(selling_price ~ .,
                    data=cars,
                    family="binomial")

vif(glm.multiple_red)
```
All the VIF values are below 5 now.

# Apply AIC for variable selection
```{r}
step(glm.multiple_red)
```
Our final model will include the predictors: km_driven + fuel + seller_type + transmission + owner + engine + max_power + age
@To-Do Full equation

#Fit logistic regression for our final model
```{r}
cars <- cars %>% select(-c(mileage, seats))

glm.multiple_final <- glm(selling_price ~ .,
                      data=cars,
                     family="binomial")

summary(glm.multiple_final)
```
Most statistical significant predictors: transmissionManual, engine, max_power, age
@To-Do Fitted equation
@To-Do Interpret coefficients

# 95% Confidence Intervals
```{r}
confint(glm.multiple_final, level = 0.95)
```
@To-Do Interpret Confidence Intervals

# Test model significance using likelihood ratio test
```{r}
#significance of full model/subset of predictors

glm.null <- glm(selling_price ~ 1,
                data=cars,
                family="binomial")

# Test for significance of the full model:
anova(glm.null, glm.multiple_final, test = "LRT")
```
The p-val of Full model is almost 0. This means that there is there is an improvement on Full model over Null. Our model is significant.

# Test for significance of specific predictors
@To-do
```{r}
# Test for significance of specific predictors
anova(glm.balance, glm.multiple, test = "LRT")
```


