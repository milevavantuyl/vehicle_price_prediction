---
title: "04 Linear Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages
```{r}
library(readr)
library(dplyr)
library(stringr)
library(tidyr)
library(tidyverse)
```

# Read Data
```{r}
# Read data
vehicles <- read_csv("data/cars_processed.csv")

# Convert chr columns to factors 
factor_column_names <- colnames(vehicles %>% select(where(is.character)))
vehicles[, factor_column_names] <- lapply(vehicles[,factor_column_names], factor)

head(vehicles)
```


# Features for linear regression model

```{r}
vehicles_lr <- vehicles %>% select(-c(year, selling_price, selling_price_binary))
head(vehicles_lr)
```

# Fitting the initial Full Multiple Linear Regression Model

```{r}
#Multiple linear regression
mlr_obj <- lm(selling_price_dollars ~ .,
             data = vehicles_lr)
summary(mlr_obj)
#full model is significant but there are some individual predictors are not significant.
```

# Checking for Collinearity 

"Make" has the greatest VIF score that is greater than 5. We remove this column.

```{r}
library(car)
vif(mlr_obj) 
```

```{r}
reduced_1 <- lm(selling_price_dollars ~ .-Make, 
             data = vehicles_lr)
```

After removing "Make", we still have a predictor that has VIF score greater than 5. We drop "engine" in next step.

```{r}
vif(reduced_1)
```

```{r}
reduced_2 <- lm(selling_price_dollars ~ .-Make-engine,
             data = vehicles_lr)
#summary(reduced_2)
```

All predictors have VIF scores less than 5 in reduced model.

```{r}
vif(reduced_2)
```

# Variable Selection via Backward AIC

After handling collinearity, we will select the most efficient predictors for our model by using AIC. 

As we can see the outcomes of step() function, we dont need to drop any other predictors since AIC wont improve more.

```{r}
step(reduced_2)
```


# Significance of Categorical Variables

I just rename our final model and check for the model significance and individual predictor significance.

```{r}
# rename reduced_2 as final_model
final_model <- lm(selling_price ~ .-Make-engine,
             data = vehicles)
summary(final_model)
```

Our model is significant based on p value of F-statistics < 0.05 but we have some predictors are not significant. 

Now, we will test the significance of the individual predictors.

**Final Model vs Model without "fuel"**

"fuel" predictor is significant.

```{r}
#without fuel
null_model1 <- lm(selling_price ~ .-Make-engine-fuel,
             data = vehicles)

#full vs without fuel
anova(final_model, null_model1)
```


**Final Model vs Model without "owner"**

"owner" predictor is significant.

```{r}
#without owner
null_model2 <- lm(selling_price ~ .-Make-engine-owner,
             data = vehicles)

#full vs without owner
anova(final_model, null_model2)
```


**Final Model vs Model without "owner+ fuel"**

"owner + fuel" predictor is significant.

```{r}
#without owner + fuel
null_model3 <- lm(selling_price ~ .-Make-engine-owner-fuel,
             data = vehicles)

#full vs without fuel + owner
anova(final_model, null_model3)
```


# Final Multiple Linear Regression

```{r}
summary(final_model)
```

# Interpretations of Full Model

#Not sure if we are interpreting the final model of Expectation 3 or final model of Expectation 4???

__RSE= 454900__

On average, out predicted selling price miss the true selling price by 454,900 ???(what is the unit for selling price).


__R-squared= 0.6842__

Our model explainss 68.42% of variability in sellinng price.



# Expectations 4
# Non-linearity check -- Mileva's part

```{r}

## Residuals-vs-Fitted Plot (the 1st diagnostic plot):
plot(final_model, which = 1)
```






