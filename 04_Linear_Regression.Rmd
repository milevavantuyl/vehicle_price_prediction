---
title: "04 Linear Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages
```{r}
library(readr)
library(dplyr)
library(stringr)
library(tidyr)
library(tidyverse)
```

# Read Data
```{r}
# Read data
vehicles <- read_csv("data/cars_processed.csv")

# Convert chr columns to factors 
factor_column_names <- colnames(vehicles %>% select(where(is.character)))
vehicles[, factor_column_names] <- lapply(vehicles[,factor_column_names], factor)

head(vehicles)
```

# Features for linear regression model

```{r}
vehicles_lr <- vehicles %>% select(-c(year, selling_price, selling_price_binary))
head(vehicles_lr)
```

# Expectation 3
## Fitting the initial Full Multiple Linear Regression Model

```{r}
#Multiple linear regression
mlr_obj <- lm(selling_price_dollars ~ .,
             data = vehicles_lr)
summary(mlr_obj)
#full model is significant but there are some individual predictors are not significant.
```

## Checking for Collinearity 

"Make" has the greatest VIF score that is greater than 5. We remove this column.

```{r}
library(car)
vif(mlr_obj) 
```

```{r}
reduced_1 <- lm(selling_price_dollars ~ .-Make, 
             data = vehicles_lr)
```

After removing "Make", we still have a predictor that has VIF score greater than 5. We drop "engine" in next step.

```{r}
vif(reduced_1)
```
```{r}
reduced_2 <- lm(selling_price_dollars ~ .-Make-engine,
             data = vehicles_lr)
#summary(reduced_2)
```

All predictors have VIF scores less than 5 in reduced model.

```{r}
vif(reduced_2)
```

## Variable Selection via Backward AIC

After handling collinearity, we will select the most efficient predictors for our model by using AIC. 

As we can see the outcomes of step() function, we dont need to drop any other predictors since AIC wont improve more.

```{r}
step(reduced_2)
```


## Significance of Categorical Variables

I just rename our final model and check for the model significance and individual predictor significance.

```{r}
# rename reduced_2 as final_model
final_model <- lm(selling_price_dollars ~ .-Make-engine,
             data = vehicles_lr)
summary(final_model)
```

Our model is significant based on p value of F-statistics < 2.2e-16 but we have some predictors are not significant. 

Now, we will test the significance of the individual predictors.

**Final Model vs Model without "fuel"**

"fuel" predictor is significant.

```{r}
#without fuel
null_model1 <- lm(selling_price_dollars ~ .-Make-engine-fuel,
             data = vehicles_lr)

#full vs without fuel
anova(final_model, null_model1)
```


**Final Model vs Model without "owner"**

"owner" predictor is significant.

```{r}
#without owner
null_model2 <- lm(selling_price_dollars ~ .-Make-engine-owner,
             data = vehicles_lr)

#full vs without owner
anova(final_model, null_model2)
```


**Final Model vs Model without "owner+ fuel"**

"owner + fuel" predictor is significant.

```{r}
#without owner + fuel
null_model3 <- lm(selling_price_dollars ~ .-Make-engine-owner-fuel,
             data = vehicles_lr)

#full vs without fuel + owner
anova(final_model, null_model3)
```

## Final Multiple Linear Regression

```{r}
summary(final_model)
```


# Expectations 4

## Non-linearity and non-constant variance check

* Not linear: The relationship between the predictors and response is not linear (i.e we observe a bend in the relationship and the points are not evenly distributed around 0)  
* Non-constant variance: there is non-constant variance (a clear funnel shape)

```{r}
## Residuals-vs-Fitted Plot (the 1st diagnostic plot):
plot(final_model, which = 1)
```

## Step 1: Attempt to Address Nonlinearity
*Unsuccessful (i.e. adding polynomial terms and interaction terms did not improve non-linearity)*

```{r}
# plot of response against individual predictors. 
plot(selling_price_dollars ~ km_driven, data = vehicles_lr)
plot(selling_price_dollars ~ mileage, data = vehicles_lr)
plot(selling_price_dollars ~ max_power, data = vehicles_lr)
plot(selling_price_dollars ~ seats, data = vehicles_lr)
plot(selling_price_dollars ~ age, data = vehicles_lr)
```

Add interaction and poly terms. We still observe significant nonlinearity and non-constant variance. 

```{r}
# seeing if adding polynomial terms helps. It doesn't
final_nonlinearity <- lm(selling_price_dollars~ .-Make-engine + mileage:km_driven + I(age^2),data = vehicles_lr)
plot(final_model, which = 1)
plot(final_nonlinearity, which = 1)
```

## Step 2: Address non-constant variance
*This attempt was successful*
Addressing non-constant variance via a log transformation of the Y variable (before adding polynomial/ interaction terms) was very successful at addressing aspects of non-linearity and non-constant variance. 

```{r}
# log transformation of response to address non-constant variance
final_logY <- lm(log(selling_price_dollars)~ .-Make-engine,
             data = vehicles_lr)
plot(final_logY , which = 1)
```

## Final model before checking for normality via qqplot

The final model after model diagnostics is as follows:
log(selling_price_dollars)~ .-Make-engine
```{r}
# final model called "final_model" with the form log(selling_price_dollars)~ .-Make-engine
final_logY <- lm(log(selling_price_dollars)~ .-Make-engine,
             data = vehicles_lr)
final_model <- final_logY

plot(final_logY, which = 1)
```

## Normality 

There are slight deviations from the normal curve. This is fine, however, since the dataset consists of n = 7906 instances. 
```{r}
plot(final_model, which = 2)
plot(density(resid(final_model)))
```

## Outlier analysis

### Regression outliers via standardized residuals

We observe various regression outliers via standardized residuals. The most extreme being observation 3378, 7521, and 5273
```{r}
plot(final_model, which=3) 
plot(rstandard(final_model))
text(x=1:nrow(vehicles_lr),y=rstandard(final_model), rownames(vehicles_lr), cex=0.6, pos=4, col="red")
```
### High-leverage points 

We observe various high-leverage points via hatvalues. We observe a few high-leverage points, the top three being 3378, 4257, and 6048. 
```{r}
plot(hatvalues(final_model))
text(x=1:nrow(vehicles_lr),y=hatvalues(final_model), rownames(vehicles_lr), cex=0.6, pos=4, col="red")
```

### Influential outliers

We observe various influential outliers via cook's distance. Instance 3378 is the main influential outlier that we will investigate further. 
```{r}
plot(final_model, which = 4)
```

## Influential outlier: Instance 3378
We observe it has km_driven 2,360,457 which is an incredibly high value (see distribution statistics and boxplot for km_driven below). That's more than 1.4million miles. That said there is nothing systematically different about this instance compared to the other instances in the data. 
```{r}
vehicles_lr[3378,]
```
```{r}
summary(vehicles_lr$km_driven)
```

```{r}
boxplot(vehicles_lr$km_driven)
```


## Test what happens if we remove the outlier

```{r}
# remove outlier
vehicles_lr_wout_outlier <- vehicles_lr[-c(3378),]

# Refit model without outlier
final_model_wout_outlier <- lm(log(selling_price_dollars)~ .-Make-engine + I(age^2),
             data = vehicles_lr_wout_outlier)
```

```{r}
# Model diagnostics after removing outlier
plot(final_model_wout_outlier, which = 1)
plot(final_model_wout_outlier, which = 2)
plot(final_model_wout_outlier, which = 4)
```

There is no significant difference in model performance (RSE/ R^2) if we exclude the outlier. 
Thus, we will not remove this influential outlier from the model.

Model performance with outlier:   

* $RSE = 0.30$ \
* $R^2 = 0.87$

Model performance without outlier:   
* $RSE = 0.30$ \
* $R^2 = 0.87$


```{r}
summary(final_model)
summary(final_model_wout_outlier)
```


# Expectations 5

## Interpretations of Full Model

The final model after model diagnostics is as follows:

* log(selling_price_dollars) ~ .-Make-engine + I(age^2) \
* We chose not to remove any outliers

```{r}
# final model
summary(final_model, data = vehicles_lr)
```


#Not sure if we are interpreting the final model of Expectation 3 or final model of Expectation 4???

__RSE= 454900__

On average, out predicted selling price miss the true selling price by 454,900 ???(what is the unit for selling price).


__R-squared= 0.6842__

Our model explainss 68.42% of variability in sellinng price.









